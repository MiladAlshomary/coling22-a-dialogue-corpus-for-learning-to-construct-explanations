{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob \n",
    "from tabulate import tabulate \n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import random\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed(123)\n",
    "np.random.seed(123)\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mace_annotation_df = pd.read_pickle('./data/annotation-results/MACE-measure/final_mace_predictions.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mace_annotation_df['turn_text_with_topic'] = mace_annotation_df.apply(lambda row: {\n",
    "                                                                        'author': row['turn_text']['author'], \n",
    "                                                                        'text' : row['topic'].replace('_', ' ') + ' [SEP] ' +  row['turn_text']['text']\n",
    "                                                                       } ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_id</th>\n",
       "      <th>turn_id</th>\n",
       "      <th>topic_func_label</th>\n",
       "      <th>dlg_act_label</th>\n",
       "      <th>exp_act_label</th>\n",
       "      <th>topic</th>\n",
       "      <th>lvl</th>\n",
       "      <th>turn_text</th>\n",
       "      <th>turn_text_with_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>142</td>\n",
       "      <td>1</td>\n",
       "      <td>(T04) Other - No topic was introduced</td>\n",
       "      <td>(D09) To provide informing statement</td>\n",
       "      <td>(E02) Testing prior knowledge</td>\n",
       "      <td>virtual_reality</td>\n",
       "      <td>colleague</td>\n",
       "      <td>{'author': 'Explainer', 'text': 'Mostly today, I've been talking a lot about what can we do, what's possible. We think might be possible in the next couple of years. But really at the professional level its more the question of wisdom of what should we be doing.'}</td>\n",
       "      <td>{'author': 'Explainer', 'text': 'virtual reality [SEP] Mostly today, I've been talking a lot about what can we do, what's possible. We think might be possible in the next couple of years. But really at the professional level its more the question of wisdom of what should we be doing.'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>142</td>\n",
       "      <td>2</td>\n",
       "      <td>(T01) It is the main topic</td>\n",
       "      <td>(D09) To provide informing statement</td>\n",
       "      <td>(E03) Provide an explanation</td>\n",
       "      <td>virtual_reality</td>\n",
       "      <td>colleague</td>\n",
       "      <td>{'author': 'Explainee', 'text': 'That's one of the things we're trying to figure out, from an artist and storytelling perspective, what are the things that will make this meaningfully different from what we're used to, like a television on our wall. And we've been finding a lot of things, aspects of virtual reality that very much do that in my opinion. Things that allow you to feel presence, first and foremost, where you get lost, and you have to remind yourself, this isn't actually happening. And things that ultimately allow you to embody other characters. Things where you can actually change your own self-perception and play with neuro-plasticity and teach yourself things that are bizarre and unique.'}</td>\n",
       "      <td>{'author': 'Explainee', 'text': 'virtual reality [SEP] That's one of the things we're trying to figure out, from an artist and storytelling perspective, what are the things that will make this meaningfully different from what we're used to, like a television on our wall. And we've been finding a lot of things, aspects of virtual reality that very much do that in my opinion. Things that allow you to feel presence, first and foremost, where you get lost, and you have to remind yourself, this isn't actually happening. And things that ultimately allow you to embody other characters. Things where you can actually change your own self-perception and play with neuro-plasticity and teach yourself things that are bizarre and unique.'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>142</td>\n",
       "      <td>3</td>\n",
       "      <td>(T03) A related topic</td>\n",
       "      <td>(D09) To provide informing statement</td>\n",
       "      <td>(E03) Provide an explanation</td>\n",
       "      <td>virtual_reality</td>\n",
       "      <td>colleague</td>\n",
       "      <td>{'author': 'Explainer', 'text': 'As an engineer of course, I love quantifiable things. I like saying here's my 18 millisecond motion to photon, here's my angular resolution that I'm improving. I'm doing the color-space right. But you can look not too far back where you say we have blu-ray DVDs at this amazing resolution, but more people want to watch Youtube videos at really bad early internet video speeds. Where there are things that if you deliver a value to people then these objective quantities may not be the most important thing. And while we're certainly pushing as hard as we can on lots of these things that make the experience better in potentially every way or maybe just for videos or for different things. I don't think that its necessary. I've commented that I think usually my favorite titles on mobile that are fully synthetic are ones that don't even try, they just go and do light-mapped, black-shaded. And I think its a lovely aesthetic. I think that you don't wind up fighting all of the aliasing. While you get some other titles that, oh we're gonna be high-tech with our specular bump maps with roughness. And you've got aliasing everywhere, and you can't hold frame rate and its all problematic. While some of these that are clearly very synthetic worlds where its nothing but these cartoony flat-shaded things with lighting, but they look and they feel good. And you can buy that you're in that place. And you want to know what's around that monolith over there.'}</td>\n",
       "      <td>{'author': 'Explainer', 'text': 'virtual reality [SEP] As an engineer of course, I love quantifiable things. I like saying here's my 18 millisecond motion to photon, here's my angular resolution that I'm improving. I'm doing the color-space right. But you can look not too far back where you say we have blu-ray DVDs at this amazing resolution, but more people want to watch Youtube videos at really bad early internet video speeds. Where there are things that if you deliver a value to people then these objective quantities may not be the most important thing. And while we're certainly pushing as hard as we can on lots of these things that make the experience better in potentially every way or maybe just for videos or for different things. I don't think that its necessary. I've commented that I think usually my favorite titles on mobile that are fully synthetic are ones that don't even try, they just go and do light-mapped, black-shaded. And I think its a lovely aesthetic. I think that you don't wind up fighting all of the aliasing. While you get some other titles that, oh we're gonna be high-tech with our specular bump maps with roughness. And you've got aliasing everywhere, and you can't hold frame rate and its all problematic. While some of these that are clearly very synthetic worlds where its nothing but these cartoony flat-shaded things with lighting, but they look and they feel good. And you can buy that you're in that place. And you want to know what's around that monolith over there.'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>142</td>\n",
       "      <td>4</td>\n",
       "      <td>(T03) A related topic</td>\n",
       "      <td>(D09) To provide informing statement</td>\n",
       "      <td>(E03) Provide an explanation</td>\n",
       "      <td>virtual_reality</td>\n",
       "      <td>colleague</td>\n",
       "      <td>{'author': 'Explainee', 'text': 'We did a project called Life of Us, which was exactly that mindset. We were let's embrace low-poly aesthetic and just simple vertex shading. And we ended up realizing, you can embody these various creatures and transform yourself. And when you do that with co-presence of another creature, another human, it makes for a totally magical journey. You don't even think for a second, you actually dismiss the whole idea of photo-realism and embrace that reality for what it is. I think it actually helps put you at ease a little bit.'}</td>\n",
       "      <td>{'author': 'Explainee', 'text': 'virtual reality [SEP] We did a project called Life of Us, which was exactly that mindset. We were let's embrace low-poly aesthetic and just simple vertex shading. And we ended up realizing, you can embody these various creatures and transform yourself. And when you do that with co-presence of another creature, another human, it makes for a totally magical journey. You don't even think for a second, you actually dismiss the whole idea of photo-realism and embrace that reality for what it is. I think it actually helps put you at ease a little bit.'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142</td>\n",
       "      <td>5</td>\n",
       "      <td>(T01) It is the main topic</td>\n",
       "      <td>(D09) To provide informing statement</td>\n",
       "      <td>(E03) Provide an explanation</td>\n",
       "      <td>virtual_reality</td>\n",
       "      <td>colleague</td>\n",
       "      <td>{'author': 'Explainer', 'text': 'The end goal of reality, of course in computer graphics people chase photo-realistic form for a long time. And basically, we've achieved it. Photo-realism, if you're willing to throw enough discreet path traced rays at things, you can generate photo-realistic views. And we understand the light really well. Of course it still takes a half hour per frame like it always has, or more, to render the different things. So its an understood problem and given infinite computing power we could be doing that in virtual reality. However, a point that I've made to people in recent years is that, we are running out of More's Law. Maybe we'll see some wonderful breakthrough in quantum structures or whatever--'}</td>\n",
       "      <td>{'author': 'Explainer', 'text': 'virtual reality [SEP] The end goal of reality, of course in computer graphics people chase photo-realistic form for a long time. And basically, we've achieved it. Photo-realism, if you're willing to throw enough discreet path traced rays at things, you can generate photo-realistic views. And we understand the light really well. Of course it still takes a half hour per frame like it always has, or more, to render the different things. So its an understood problem and given infinite computing power we could be doing that in virtual reality. However, a point that I've made to people in recent years is that, we are running out of More's Law. Maybe we'll see some wonderful breakthrough in quantum structures or whatever--'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   task_id  turn_id                       topic_func_label  \\\n",
       "0      142        1  (T04) Other - No topic was introduced   \n",
       "1      142        2             (T01) It is the main topic   \n",
       "2      142        3                  (T03) A related topic   \n",
       "3      142        4                  (T03) A related topic   \n",
       "4      142        5             (T01) It is the main topic   \n",
       "\n",
       "                          dlg_act_label                  exp_act_label  \\\n",
       "0  (D09) To provide informing statement  (E02) Testing prior knowledge   \n",
       "1  (D09) To provide informing statement   (E03) Provide an explanation   \n",
       "2  (D09) To provide informing statement   (E03) Provide an explanation   \n",
       "3  (D09) To provide informing statement   (E03) Provide an explanation   \n",
       "4  (D09) To provide informing statement   (E03) Provide an explanation   \n",
       "\n",
       "             topic        lvl  \\\n",
       "0  virtual_reality  colleague   \n",
       "1  virtual_reality  colleague   \n",
       "2  virtual_reality  colleague   \n",
       "3  virtual_reality  colleague   \n",
       "4  virtual_reality  colleague   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               turn_text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {'author': 'Explainer', 'text': 'Mostly today, I've been talking a lot about what can we do, what's possible. We think might be possible in the next couple of years. But really at the professional level its more the question of wisdom of what should we be doing.'}   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              {'author': 'Explainee', 'text': 'That's one of the things we're trying to figure out, from an artist and storytelling perspective, what are the things that will make this meaningfully different from what we're used to, like a television on our wall. And we've been finding a lot of things, aspects of virtual reality that very much do that in my opinion. Things that allow you to feel presence, first and foremost, where you get lost, and you have to remind yourself, this isn't actually happening. And things that ultimately allow you to embody other characters. Things where you can actually change your own self-perception and play with neuro-plasticity and teach yourself things that are bizarre and unique.'}   \n",
       "2  {'author': 'Explainer', 'text': 'As an engineer of course, I love quantifiable things. I like saying here's my 18 millisecond motion to photon, here's my angular resolution that I'm improving. I'm doing the color-space right. But you can look not too far back where you say we have blu-ray DVDs at this amazing resolution, but more people want to watch Youtube videos at really bad early internet video speeds. Where there are things that if you deliver a value to people then these objective quantities may not be the most important thing. And while we're certainly pushing as hard as we can on lots of these things that make the experience better in potentially every way or maybe just for videos or for different things. I don't think that its necessary. I've commented that I think usually my favorite titles on mobile that are fully synthetic are ones that don't even try, they just go and do light-mapped, black-shaded. And I think its a lovely aesthetic. I think that you don't wind up fighting all of the aliasing. While you get some other titles that, oh we're gonna be high-tech with our specular bump maps with roughness. And you've got aliasing everywhere, and you can't hold frame rate and its all problematic. While some of these that are clearly very synthetic worlds where its nothing but these cartoony flat-shaded things with lighting, but they look and they feel good. And you can buy that you're in that place. And you want to know what's around that monolith over there.'}   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {'author': 'Explainee', 'text': 'We did a project called Life of Us, which was exactly that mindset. We were let's embrace low-poly aesthetic and just simple vertex shading. And we ended up realizing, you can embody these various creatures and transform yourself. And when you do that with co-presence of another creature, another human, it makes for a totally magical journey. You don't even think for a second, you actually dismiss the whole idea of photo-realism and embrace that reality for what it is. I think it actually helps put you at ease a little bit.'}   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {'author': 'Explainer', 'text': 'The end goal of reality, of course in computer graphics people chase photo-realistic form for a long time. And basically, we've achieved it. Photo-realism, if you're willing to throw enough discreet path traced rays at things, you can generate photo-realistic views. And we understand the light really well. Of course it still takes a half hour per frame like it always has, or more, to render the different things. So its an understood problem and given infinite computing power we could be doing that in virtual reality. However, a point that I've made to people in recent years is that, we are running out of More's Law. Maybe we'll see some wonderful breakthrough in quantum structures or whatever--'}   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          turn_text_with_topic  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {'author': 'Explainer', 'text': 'virtual reality [SEP] Mostly today, I've been talking a lot about what can we do, what's possible. We think might be possible in the next couple of years. But really at the professional level its more the question of wisdom of what should we be doing.'}  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              {'author': 'Explainee', 'text': 'virtual reality [SEP] That's one of the things we're trying to figure out, from an artist and storytelling perspective, what are the things that will make this meaningfully different from what we're used to, like a television on our wall. And we've been finding a lot of things, aspects of virtual reality that very much do that in my opinion. Things that allow you to feel presence, first and foremost, where you get lost, and you have to remind yourself, this isn't actually happening. And things that ultimately allow you to embody other characters. Things where you can actually change your own self-perception and play with neuro-plasticity and teach yourself things that are bizarre and unique.'}  \n",
       "2  {'author': 'Explainer', 'text': 'virtual reality [SEP] As an engineer of course, I love quantifiable things. I like saying here's my 18 millisecond motion to photon, here's my angular resolution that I'm improving. I'm doing the color-space right. But you can look not too far back where you say we have blu-ray DVDs at this amazing resolution, but more people want to watch Youtube videos at really bad early internet video speeds. Where there are things that if you deliver a value to people then these objective quantities may not be the most important thing. And while we're certainly pushing as hard as we can on lots of these things that make the experience better in potentially every way or maybe just for videos or for different things. I don't think that its necessary. I've commented that I think usually my favorite titles on mobile that are fully synthetic are ones that don't even try, they just go and do light-mapped, black-shaded. And I think its a lovely aesthetic. I think that you don't wind up fighting all of the aliasing. While you get some other titles that, oh we're gonna be high-tech with our specular bump maps with roughness. And you've got aliasing everywhere, and you can't hold frame rate and its all problematic. While some of these that are clearly very synthetic worlds where its nothing but these cartoony flat-shaded things with lighting, but they look and they feel good. And you can buy that you're in that place. And you want to know what's around that monolith over there.'}  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {'author': 'Explainee', 'text': 'virtual reality [SEP] We did a project called Life of Us, which was exactly that mindset. We were let's embrace low-poly aesthetic and just simple vertex shading. And we ended up realizing, you can embody these various creatures and transform yourself. And when you do that with co-presence of another creature, another human, it makes for a totally magical journey. You don't even think for a second, you actually dismiss the whole idea of photo-realism and embrace that reality for what it is. I think it actually helps put you at ease a little bit.'}  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {'author': 'Explainer', 'text': 'virtual reality [SEP] The end goal of reality, of course in computer graphics people chase photo-realistic form for a long time. And basically, we've achieved it. Photo-realism, if you're willing to throw enough discreet path traced rays at things, you can generate photo-realistic views. And we understand the light really well. Of course it still takes a half hour per frame like it always has, or more, to render the different things. So its an understood problem and given infinite computing power we could be doing that in virtual reality. However, a point that I've made to people in recent years is that, we are running out of More's Law. Maybe we'll see some wonderful breakthrough in quantum structures or whatever--'}  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mace_annotation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(T04) Other - No topic was introduced    772\n",
       "(T01) It is the main topic               430\n",
       "(T03) A related topic                    260\n",
       "(T02) A subtopic                          88\n",
       "Name: topic_func_label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mace_annotation_df.topic_func_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(D09) To provide informing statement           696\n",
       "(D07) To provide agreement statement           265\n",
       "(D01) To ask a check question                  245\n",
       "(D02) To ask what/how question                 115\n",
       "(D10) Other                                    101\n",
       "(D04) To answer a question by confirming        54\n",
       "(D06) To answer - Other                         25\n",
       "(D05) To answer a question by disconfirming     24\n",
       "(D03) To ask other kind of questions            13\n",
       "(D08) To provide disagreement statement         12\n",
       "Name: dlg_act_label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mace_annotation_df.dlg_act_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(E03) Provide an explanation                679\n",
       "(E07) Providing Feedback                    285\n",
       "(E04) Ask for an explanation                142\n",
       "(E05) Signaling understanding               141\n",
       "(E02) Testing prior knowledge               112\n",
       "(E10) Other                                  59\n",
       "(E01) Testing understanding                  56\n",
       "(E09) Introducing Extraneous Information     48\n",
       "(E06) Signaling non-understanding            17\n",
       "(E08) Providing Assessment                   11\n",
       "Name: exp_act_label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mace_annotation_df.exp_act_label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models to experiment with:\n",
    "- Majority label baseline\n",
    "- Basic BERT model\n",
    "- A model that expose the dependency between the label\n",
    "- A model that takes into consideration labels of other sequences in the dialog.\n",
    "\n",
    "### How to evaluate:\n",
    "- Cross domain evaluation: for each topic train the model on all other topics and evaluate on the selected topic (13-fold cross validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Majority class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_class(df):\n",
    "    topics = df.topic.unique()\n",
    "    for topic in topics:\n",
    "        training_df = df[df.topic != topic]\n",
    "        #compute the majority class for each label\n",
    "        l = len(df[df.topic == topic])\n",
    "        df.loc[df.topic == topic, 'topic_func_maj_pred'] = [training_df.topic_func_label.mode()] * l\n",
    "        df.loc[df.topic == topic, 'dlg_act_maj_pred']    = [training_df.dlg_act_label.mode()] * l\n",
    "        df.loc[df.topic == topic, 'exp_act_maj_pred']    = [training_df.exp_act_label.mode()] * l\n",
    "    \n",
    "    return df\n",
    "\n",
    "def eval_preds(df, models_names, gt_clms, pred_clms):\n",
    "    results_table = []\n",
    "    for label in zip(gt_clms, pred_clms, models_names):\n",
    "        ground_truths = df[label[0]].tolist()\n",
    "        predictions   = df[label[1]].tolist()\n",
    "        model_name = label[2]\n",
    "        \n",
    "        class_names = df[label[0]].unique()\n",
    "\n",
    "        prc_scores = precision_score(ground_truths, predictions, average=None, labels=class_names)\n",
    "        rec_scores = recall_score(ground_truths, predictions, average=None, labels=class_names)\n",
    "        f1_scores  = f1_score(ground_truths, predictions, average=None, labels=class_names)\n",
    "        \n",
    "        macro_prc_scores = precision_score(predictions, ground_truths, average='macro', labels=class_names)\n",
    "        macro_rec_scores = recall_score(predictions, ground_truths, average='macro', labels=class_names)\n",
    "        macro_f1 = f1_score(predictions, ground_truths, average='macro', labels=class_names)\n",
    "        \n",
    "        scores ={}\n",
    "        for i, c in enumerate(class_names):\n",
    "            scores[c] = {'prec': round(prc_scores[i],2), 'recall': round(rec_scores[i],2), 'f1': round(f1_scores[i],2)}\n",
    "        \n",
    "        scores['Macro AVG.'] = {'prec': round(macro_prc_scores,2), 'recall': round(macro_rec_scores,2), 'f1': round(macro_f1,2)}\n",
    "        \n",
    "        results_table.append([model_name, label[0], scores])\n",
    "    \n",
    "    return results_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset, Dataset, load_metric\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, default_data_collator,\n",
    "                          PreTrainedModel, BertModel, BertForSequenceClassification,\n",
    "                          TrainingArguments, Trainer)\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tabulate import tabulate\n",
    "from glob import glob\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(-1)\n",
    "    f1score = f1_score(predictions, labels, average='macro')\n",
    "    return {'f1-score': f1score}\n",
    "\n",
    "def preprocess_function(examples, input_clm='turn_text'):\n",
    "    # Tokenize the texts\n",
    "    texts = [x['text'] for x in examples[input_clm]]\n",
    "    result = tokenizer(texts, truncation=True, padding='max_length')\n",
    "    \n",
    "    if 'labels' in examples:\n",
    "        result['labels'] = examples['labels']\n",
    "        \n",
    "    return result\n",
    "\n",
    "def compute_model_scores(model, eval_dataset, class_names):\n",
    "    eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n",
    "    eval_dataset = eval_dataset.remove_columns(['__index_level_0__', 'dlg_act_label', 'exp_act_label', 'lvl', 'task_id', 'topic', 'topic_func_label', 'turn_id', 'turn_text'])\n",
    "\n",
    "    eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator)\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        batch = {x[0]: x[1].cuda() for x in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        all_predictions+=[x.item() for x in predictions]\n",
    "        all_labels+=[x.item() for x in batch['labels']]\n",
    "\n",
    "    prc_scores = precision_score(all_predictions, all_labels, average=None, labels=class_names)\n",
    "    rec_scores = recall_score(all_predictions, all_labels, average=None, labels=class_names)\n",
    "    f1_scores  = f1_score(all_predictions, all_labels, average=None, labels=class_names)\n",
    "    \n",
    "    return prc_scores, rec_scores, f1_scores\n",
    "\n",
    "def bert_pred_labels(models_path, df, label_clm, input_clm):\n",
    "    label_dictionary = {int(l[2:4])-1 : l for l in  df[label_clm].unique()}\n",
    "    \n",
    "    df[label_clm + '_bert_pred'] = [None] * len(df)\n",
    "    \n",
    "    class_names = list(sorted(df[label_clm]))\n",
    "    \n",
    "    for model_path in glob(models_path):\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "        eval_topic = model_path.split('/')[-1].replace('model_', '')\n",
    "        print(eval_topic)\n",
    "        eval_dataset = Dataset.from_pandas(df[df.topic == eval_topic])\n",
    "        eval_dataset = eval_dataset.map(lambda examples: preprocess_function(examples, input_clm), batched=True)\n",
    "        eval_dataset = eval_dataset.remove_columns(df.columns.tolist() + ['__index_level_0__'])\n",
    "        #eval_dataset = eval_dataset.remove_columns(['__index_level_0__', 'dlg_act_label', 'exp_act_label', 'lvl', 'task_id', 'topic', 'topic_func_label', 'turn_id', input_clm])\n",
    "\n",
    "        eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator)\n",
    "        all_predictions = []\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            batch = {x[0]: x[1].cuda() for x in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            all_predictions+=[x.item() for x in predictions]\n",
    "\n",
    "        df.loc[df.topic == eval_topic,label_clm + '_bert_pred'] = [label_dictionary[x] for x in all_predictions]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(output_dir, train_ds, valid_ds, test_ds, num_labels , num_train_epochs=5):\n",
    "    batch_size = 4\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir= output_dir,\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        save_strategy = \"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='f1-score'\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels).to(device)\n",
    "    \n",
    "    multi_trainer =Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=valid_ds,\n",
    "        compute_metrics=lambda x: compute_metrics(x),\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    multi_trainer.train()\n",
    "    \n",
    "    model.save_pretrained(output_dir)\n",
    "    eval_results = multi_trainer.evaluate(test_ds)\n",
    "    \n",
    "    return model, eval_results\n",
    "\n",
    "def cross_topic_eval(df, label_clm, input_clm, output_dir, num_train_epochs=5):\n",
    "    df['labels'] = df[label_clm].apply(lambda x: int(x[2:4])-1) #making labels parasable as integers\n",
    "    num_labels = df['labels'].nunique()\n",
    "    \n",
    "    topics = df.topic.unique() #Train and evaluate one model for each topic...\n",
    "    all_eval_results = {}\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "        training_df= df[df.topic != topic]\n",
    "        \n",
    "        #split the training df to further training and validation per topic\n",
    "        train_topics = training_df.topic.unique()\n",
    "        train_topics, test_topics = train_test_split(train_topics, test_size=2)\n",
    "        \n",
    "        new_training_df = training_df[training_df.topic.isin(train_topics)].copy()\n",
    "        valid_df    = training_df[training_df.topic.isin(test_topics)].copy()\n",
    "        test_df     = df[df.topic == topic].copy()\n",
    "        \n",
    "        #balance the data\n",
    "        new_training_df, y = ros.fit_resample(new_training_df, new_training_df['labels'])\n",
    "        new_training_df['labels'] = y\n",
    "        \n",
    "        train_ds = Dataset.from_pandas(new_training_df)\n",
    "        valid_ds = Dataset.from_pandas(valid_df)\n",
    "        test_ds  = Dataset.from_pandas(test_df)\n",
    "        \n",
    "        train_ds = train_ds.map(lambda examples: tokenizer([x['text'] for x in examples[input_clm]], truncation=True, padding='max_length'), batched=True)\n",
    "        valid_ds = valid_ds.map(lambda examples: tokenizer([x['text'] for x in examples[input_clm]], truncation=True, padding='max_length'), batched=True)\n",
    "        test_ds  = test_ds.map(lambda examples: tokenizer([x['text'] for x in examples[input_clm]], truncation=True, padding='max_length'), batched=True)\n",
    "        \n",
    "        eval_results = train_model(output_dir + '_{}'.format(topic), train_ds, valid_ds, test_ds, num_labels, num_train_epochs=num_train_epochs)\n",
    "        \n",
    "        all_eval_results[topic] = eval_results\n",
    "\n",
    "    return all_eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each topic, train a model on the other 12 topics as follows:\n",
    "  - From the 12 topics select one for validation and 11 for training\n",
    "  - Train the model for 10 epochs selecting the checkpoint that gives the highest f1-score on the validation set\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train for the topic_func_label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virtual_reality\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d260562987b54ab68716a419cf1947ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0901fc6471c4412b9762fef18b4bf02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "032e47c5cfae40ec828b9c095b197fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: dlg_act_label, exp_act_label, turn_text, turn_id, topic_func_label, lvl, topic, task_id, turn_text_with_topic. If dlg_act_label, exp_act_label, turn_text, turn_id, topic_func_label, lvl, topic, task_id, turn_text_with_topic are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 2416\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3020\n"
     ]
    }
   ],
   "source": [
    "eval_results = cross_topic_eval(mace_annotation_df.copy(), 'topic_func_label', 'turn_text_with_topic', './data/models/topic_func_models/model', num_train_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_table = []\n",
    "for topic, res in eval_results.items():\n",
    "    res_table.append([topic, res[1]['eval_f1-score']])\n",
    "\n",
    "res_table.append(['Average', np.mean([x[1] for x in res_table])])\n",
    "\n",
    "print(tabulate(res_table, headers=['topic', 'f1-score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train for the dlg_act_label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_results = cross_topic_eval(mace_annotation_df.copy(), 'dlg_act_label', 'turn_text', './data/models/dlg_act_label/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_table = []\n",
    "for topic, res in eval_results.items():\n",
    "    res_table.append([topic, res[1]['eval_f1-score']])\n",
    "\n",
    "res_table.append(['Average', np.mean([x[1] for x in res_table])])\n",
    "\n",
    "print(tabulate(res_table, headers=['topic', 'f1-score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train for the exp_act_label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_results = cross_topic_eval(mace_annotation_df.copy(), 'exp_act_label', 'turn_text', './data/models/exp_act_label/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_table = []\n",
    "for topic, res in eval_results.items():\n",
    "    res_table.append([topic, res[1]['eval_f1-score']])\n",
    "\n",
    "res_table.append(['Average', np.mean([x[1] for x in res_table])])\n",
    "\n",
    "print(tabulate(res_table, headers=['topic', 'f1-score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction using the trained models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate predictions using majority class and our trained baseline models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['task_id', 'turn_id', 'topic_func_label', 'dlg_act_label',\n",
       "       'exp_act_label', 'topic', 'lvl', 'turn_text', 'turn_text_with_topic',\n",
       "       'topic_func_label_bert_pred'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mace_annotation_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blackhole\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3780f4d50d149eca615f75bf7e3f287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleep_scientist\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055ea9c6818a446184f91fee1ee3d51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blockchain\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8b0a55039b412eb122a690ac6fae62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virtual_reality\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11905c3b9d54589b481c817c3ee5491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origani\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2d36bc3e90450e960cc05426c3e020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nano_technology\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f819ededb6784506b5ec47e83832b29b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "music_harmony\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd735a5fab4d4c4fb844ad4bbac2a79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gravity\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f236aa831b4295b5383dffc8ee46fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine_learning\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0bf52561e764385857113ce1abe02d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497ffcbf90a94bfbac62af9f3c60fbf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hacking\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f41441f385641cf8b11f22a59efe707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f925f997a164a44993a38d8844ed14c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connectome\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd7a67d4bff4fcab9d83561d9912e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blackhole\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5239756177404298cdb2fb0a37126e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleep_scientist\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f134cb81caef4c8ca8a4aba7e9e605ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blockchain\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda5f85ce35a4be98b2233da9c30b34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virtual_reality\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e280f611b4c649a99a6b810940d841ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origani\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06055ad1c4cf4d5082164ec14224f60a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nano_technology\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6fe4aa2a5f240239b5de58ccc7bbf28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "music_harmony\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76974c416e614f788105712042077e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gravity\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2dd8483e5d248ecb2ed9078c67c1442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine_learning\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b701a80eb4f498d82328ef937a4723d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b723f080aca446148b732099236a3497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hacking\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd5c5212118420b855c7fa7f4d38786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094bff498eb5411e825009e6150fb1ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connectome\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68982de6a98f41d7837566861601b02c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blackhole\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73357196e0db4e769826a724e29d67a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleep_scientist\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df89fd49fe9474cb212a22bd2161fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blockchain\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99ec0b4d5034063b131a4d3354553a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virtual_reality\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc0496699b0497ba3723ddd76f72afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origani\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202284eafdef493988226c2db0e3a2d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nano_technology\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797ab5160f6e49adbdd10f045a13a4a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "music_harmony\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560a6aa24460476381e0b2bd5fa8e9f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gravity\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c37dc2118a4bd48114853199d20531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine_learning\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fab8fb8df8f47a49fe92bade67997df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b3b2ad8e874096a31e86538ab4def1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hacking\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af05fd802f24e85a16eab73f910ef9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83220d0ef14142d9a4bb82e98ee9f0b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connectome\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1da2bee743e457dbe88de56426d44b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mace_annotation_df = bert_pred_labels('./data/models/topic_func_models/*', mace_annotation_df, 'topic_func_label', 'turn_text_with_topic')\n",
    "mace_annotation_df = bert_pred_labels('./data/models/exp_act_label/*', mace_annotation_df, 'exp_act_label', 'turn_text')\n",
    "mace_annotation_df = bert_pred_labels('./data/models/dlg_act_label/*', mace_annotation_df, 'dlg_act_label', 'turn_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mace_annotation_df = majority_class(mace_annotation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mace_annotation_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fedc8f63ae3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmace_annotation_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./majority_and_basic_bert_pred.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mace_annotation_df' is not defined"
     ]
    }
   ],
   "source": [
    "mace_annotation_df.to_pickle('./majority_and_basic_bert_pred.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
