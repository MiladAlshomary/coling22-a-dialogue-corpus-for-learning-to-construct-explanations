{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tabulate import tabulate \n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mace_annotation_df = pd.read_pickle('./data/annotation-results/MACE-measure/final_mace_predictions.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(E03) Provide an explanation                679\n",
       "(E07) Providing Feedback                    285\n",
       "(E04) Ask for an explanation                142\n",
       "(E05) Signaling understanding               141\n",
       "(E02) Testing prior knowledge               112\n",
       "(E10) Other                                  59\n",
       "(E01) Testing understanding                  56\n",
       "(E09) Introducing Extraneous Information     48\n",
       "(E06) Signaling non-understanding            17\n",
       "(E08) Providing Assessment                   11\n",
       "Name: exp_act_label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mace_annotation_df.exp_act_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_preds(df, models_names, gt_clms, pred_clms):\n",
    "    results_table = []\n",
    "    for label in zip(gt_clms, pred_clms, models_names):\n",
    "        ground_truths = df[label[0]].tolist()\n",
    "        predictions   = df[label[1]].tolist()\n",
    "        model_name = label[2]\n",
    "        \n",
    "        class_names = df[label[0]].unique()\n",
    "\n",
    "        prc_scores = precision_score(ground_truths, predictions, average=None, labels=class_names)\n",
    "        rec_scores = recall_score(ground_truths, predictions, average=None, labels=class_names)\n",
    "        f1_scores  = f1_score(ground_truths, predictions, average=None, labels=class_names)\n",
    "        \n",
    "        macro_prc_scores = precision_score(predictions, ground_truths, average='macro', labels=class_names)\n",
    "        macro_rec_scores = recall_score(predictions, ground_truths, average='macro', labels=class_names)\n",
    "        macro_f1 = f1_score(predictions, ground_truths, average='macro', labels=class_names)\n",
    "        \n",
    "        scores ={}\n",
    "        for i, c in enumerate(class_names):\n",
    "            scores[c] = {'prec': round(prc_scores[i],2), 'recall': round(rec_scores[i],2), 'f1': round(f1_scores[i],2)}\n",
    "        \n",
    "        scores['Macro AVG.'] = {'prec': round(macro_prc_scores,2), 'recall': round(macro_rec_scores,2), 'f1': round(macro_f1,2)}\n",
    "        \n",
    "        results_table.append([model_name, label[0], scores])\n",
    "    \n",
    "    return results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mace_annotation_with_seq_preds_df = pd.read_pickle('./model_sequence_labeling/sequence_labeling_preds.pkl')\n",
    "\n",
    "mace_annotation_with_mt_preds_df  = pd.read_pickle('./multi_task_learning/mt_final_preds.pkl')\n",
    "mace_annotation_with_mt_preds_df.drop(['topic_func_label', 'dlg_act_label', 'exp_act_label', 'topic'],axis=1, inplace=True)\n",
    "\n",
    "mace_annotatoin_with_basic_per_preds_df = pd.read_pickle('./majority_and_basic_bert_pred.pkl')\n",
    "mace_annotatoin_with_basic_per_preds_df.drop(['topic_func_label', 'dlg_act_label', 'exp_act_label', 'topic'],axis=1, inplace=True)\n",
    "\n",
    "mace_annotation_preds = pd.merge(mace_annotation_with_seq_preds_df, mace_annotatoin_with_basic_per_preds_df, on=['task_id', 'turn_id'])\n",
    "mace_annotation_preds = pd.merge(mace_annotation_preds, mace_annotation_with_mt_preds_df, on=['task_id', 'turn_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores = ['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app         (T01) It is the main topic    (T02) A subtopic    (T03) A related topic    (T04) Other - No topic was introduced    Macro Avg\n",
      "--------  ----------------------------  ------------------  -----------------------  ---------------------------------------  -----------\n",
      "Majority                          0                   0                        0                                        0.66         0.17\n",
      "BERT                              0.58                0.11                     0.44                                     0.89         0.51\n",
      "BERT-SEQ                          0.61                0.13                     0.44                                     0.89         0.52\n",
      "BERT-MT                           0.39                0.13                     0.32                                     0.73         0.39\n"
     ]
    }
   ],
   "source": [
    "results_table = eval_preds(mace_annotation_preds, ['Majority', 'BERT', 'BERT-SEQ', 'BERT-MT'], ['topic_func_label', 'topic_func_label', 'topic_func_label', 'topic_func_label'], \n",
    "                           ['topic_func_maj_pred', 'topic_func_label_bert_pred', 'topic_func_label_seq_bert_pred', 'topic-func-lable_mt_pred'])\n",
    "class_names = sorted(mace_annotation_preds['topic_func_label'].unique().tolist())\n",
    "\n",
    "print(tabulate([[r[0]] + [r[2][class_name][m] for class_name in class_names + ['Macro AVG.'] for m in print_scores] for r in results_table],\n",
    "     headers=['app']+ class_names + ['Macro Avg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app         (D01)    (D02)    (D03)    (D04)    (D05)    (D06)    (D07)    (D08)    (D09)    (D10)    Macro Avg\n",
      "--------  -------  -------  -------  -------  -------  -------  -------  -------  -------  -------  -----------\n",
      "Majority     0        0        0        0        0        0        0           0     0.62     0            0.06\n",
      "BERT         0.76     0.73     0        0.33     0.67     0        0.51        0     0.87     0.57         0.44\n",
      "BERT-SEQ     0.76     0.72     0        0.35     0.67     0        0.69        0     0.87     0.61         0.47\n",
      "BERT-MT      0.43     0.46     0.06     0.18     0.19     0.03     0.24        0     0.78     0.23         0.26\n"
     ]
    }
   ],
   "source": [
    "results_table = eval_preds(mace_annotation_preds, ['Majority', 'BERT', 'BERT-SEQ', 'BERT-MT'], ['dlg_act_label', 'dlg_act_label', 'dlg_act_label', 'dlg_act_label'], \n",
    "                           ['dlg_act_maj_pred', 'dlg_act_label_bert_pred', 'dlg_act_label_seq_bert_pred', 'dlg-act-lable_mt_pred'])\n",
    "class_names = sorted(mace_annotation_preds['dlg_act_label'].unique().tolist())\n",
    "\n",
    "print(tabulate([[r[0]] + [r[2][class_name][s] for class_name in class_names + ['Macro AVG.'] for s in print_scores] for r in results_table],\n",
    "     headers=['app']+[x[0:5] for x in class_names]+ ['Macro Avg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app         (E01)    (E02)    (E03)    (E04)    (E05)    (E06)    (E07)    (E08)    (E09)    (E10)    Macro Avg\n",
      "--------  -------  -------  -------  -------  -------  -------  -------  -------  -------  -------  -----------\n",
      "Majority     0        0        0.61     0        0        0        0        0        0        0            0.06\n",
      "BERT         0.27     0.64     0.84     0.6      0.29     0.34     0.51     0        0.11     0.5          0.41\n",
      "BERT-SEQ     0.27     0.64     0.84     0.64     0.33     0.21     0.6      0.15     0.08     0.56         0.43\n",
      "BERT-SEQ     0.14     0.28     0.78     0.3      0.12     0.07     0.23     0        0.04     0.16         0.21\n"
     ]
    }
   ],
   "source": [
    "results_table = eval_preds(mace_annotation_preds, ['Majority', 'BERT', 'BERT-SEQ', 'BERT-SEQ'], ['exp_act_label', 'exp_act_label', 'exp_act_label', 'exp_act_label'], \n",
    "                           ['exp_act_maj_pred', 'exp_act_label_bert_pred', 'exp_act_label_seq_bert_pred', 'exp-act-lable_mt_pred'])\n",
    "class_names = sorted(mace_annotation_preds['exp_act_label'].unique().tolist())\n",
    "\n",
    "print(tabulate([[r[0]] + [r[2][class_name][s] for class_name in class_names + ['Macro AVG.'] for s in print_scores] for r in results_table],\n",
    "     headers=['app']+[x[0:5] for x in class_names]+ ['Macro Avg']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute significancy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sig(v1s, v2s, alpha=0.05):\n",
    "    from scipy import stats\n",
    "    from statsmodels.stats import weightstats \n",
    "\n",
    "    diff = list(map(lambda x1 , x2: x1 - x2, v1s, v2s))\n",
    "    is_normal = stats.shapiro(diff)[1] > alpha\n",
    "    \n",
    "    if is_normal:\n",
    "        #print('Distribution is normal, so using ttest_rel')\n",
    "        tstat, pvalue, df = weightstats.ttest_ind(v1s, v2s, alternative='larger')\n",
    "        #print(tstat, pvalue)\n",
    "        if tstat >=0:\n",
    "            if (pvalue) <= alpha:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    else:\n",
    "        #print('Distribution is not normal, so using wilcoxon')\n",
    "        ttest = stats.wilcoxon(v1s, v2s, alternative='greater')\n",
    "        \n",
    "        if ttest.statistic >=0:\n",
    "            if (ttest.pvalue) <= alpha:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Topic labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#1. compute f1 scores for 20 slices of the dataframe\n",
    "#2. extract the f1 scores from the results\n",
    "#3. compute significancy\n",
    "\n",
    "results_table = [\n",
    "    eval_preds(df_slice, ['Majority', 'BERT', 'BERT-SEQ', 'BERT-MT'], ['topic_func_label', 'topic_func_label', 'topic_func_label', 'topic_func_label'], \n",
    "                           ['topic_func_maj_pred', 'topic_func_label_bert_pred', 'topic_func_label_seq_bert_pred', 'topic-func-lable_mt_pred'])\n",
    "for df_slice in np.array_split(mace_annotation_preds, 5)]\n",
    "\n",
    "bert_seq_res   = [{x[0]: x[1]['f1'] for x in res_item[2][2].items()} for res_item in results_table]\n",
    "bert_basic_res = [{x[0]: x[1]['f1'] for x in res_item[1][2].items()} for res_item in results_table]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- (T04) Other - No topic was introduced -----\n",
      "[0.87, 0.91, 0.87, 0.93, 0.87]\n",
      "[0.87, 0.89, 0.88, 0.91, 0.88]\n",
      "Significancy for (T04) Other - No topic was introduced is False\n",
      "----- (T01) It is the main topic -----\n",
      "[0.6, 0.6, 0.77, 0.24, 0.52]\n",
      "[0.61, 0.72, 0.72, 0.37, 0.54]\n",
      "Significancy for (T01) It is the main topic is False\n",
      "----- (T03) A related topic -----\n",
      "[0.3, 0.46, 0.48, 0.5, 0.41]\n",
      "[0.38, 0.4, 0.49, 0.53, 0.35]\n",
      "Significancy for (T03) A related topic is False\n",
      "----- (T02) A subtopic -----\n",
      "[0.0, 0.11, 0.19, 0.14, 0.11]\n",
      "[0.23, 0.17, 0.18, 0.0, 0.0]\n",
      "Significancy for (T02) A subtopic is False\n",
      "----- Macro AVG. -----\n",
      "[0.44, 0.52, 0.58, 0.45, 0.48]\n",
      "[0.52, 0.54, 0.57, 0.45, 0.44]\n",
      "Significancy for Macro AVG. is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/scipy/stats/morestats.py:2957: UserWarning: Exact p-value calculation does not work if there are ties. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/stats/morestats.py:2971: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    }
   ],
   "source": [
    "for clm in list(bert_basic_res[0].keys()):\n",
    "    bert_seq_vals = [v[clm] for v in bert_seq_res]\n",
    "    bert_basic_vals = [v[clm] for v in bert_basic_res]\n",
    "    print('----- {} -----'.format(clm))\n",
    "    print(bert_basic_vals)\n",
    "    print(bert_seq_vals)\n",
    "    print('Significancy for {} is {}'.format(clm, check_sig(bert_seq_vals, bert_basic_vals, alpha=0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Dialogue label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "results_table = [\n",
    "    eval_preds(df_slice, ['Majority', 'BERT', 'BERT-SEQ', 'BERT-MT'], ['dlg_act_label', 'dlg_act_label', 'dlg_act_label', 'dlg_act_label'], \n",
    "                           ['dlg_act_maj_pred', 'dlg_act_label_bert_pred', 'dlg_act_label_seq_bert_pred', 'dlg-act-lable_mt_pred'])\n",
    "    for df_slice in np.array_split(mace_annotation_preds, 5)]\n",
    "\n",
    "bert_seq_res   = [{x[0]: x[1]['f1'] for x in res_item[2][2].items()} for res_item in results_table]\n",
    "bert_basic_res = [{x[0]: x[1]['f1'] for x in res_item[1][2].items()} for res_item in results_table]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- (D09) To provide informing statement -----\n",
      "[0.93, 0.84, 0.88, 0.88, 0.81]\n",
      "[0.9, 0.84, 0.86, 0.89, 0.82]\n",
      "Significancy for (D09) To provide informing statement is False\n",
      "----- (D07) To provide agreement statement -----\n",
      "[0.59, 0.46, 0.49, 0.59, 0.43]\n",
      "[0.74, 0.7, 0.63, 0.79, 0.6]\n",
      "Significancy for (D07) To provide agreement statement is True\n",
      "----- (D01) To ask a check question -----\n",
      "[0.72, 0.68, 0.8, 0.75, 0.82]\n",
      "[0.7, 0.7, 0.77, 0.77, 0.81]\n",
      "Significancy for (D01) To ask a check question is False\n",
      "----- (D05) To answer a question by disconfirming -----\n",
      "[0.67, 0.75, 0.84, 0.67, 0.33]\n",
      "[0.67, 0.75, 0.78, 0.67, 0.46]\n",
      "Significancy for (D05) To answer a question by disconfirming is False\n",
      "----- (D02) To ask what/how question -----\n",
      "[0.71, 0.74, 0.81, 0.65, 0.72]\n",
      "[0.73, 0.77, 0.74, 0.62, 0.71]\n",
      "Significancy for (D02) To ask what/how question is False\n",
      "----- (D06) To answer - Other -----\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Significancy for (D06) To answer - Other is False\n",
      "----- (D04) To answer a question by confirming -----\n",
      "[0.63, 0.32, 0.25, 0.3, 0.3]\n",
      "[0.33, 0.48, 0.33, 0.4, 0.27]\n",
      "Significancy for (D04) To answer a question by confirming is False\n",
      "----- (D10) Other -----\n",
      "[0.84, 0.62, 0.54, 0.27, 0.37]\n",
      "[0.8, 0.71, 0.48, 0.51, 0.21]\n",
      "Significancy for (D10) Other is False\n",
      "----- (D03) To ask other kind of questions -----\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Significancy for (D03) To ask other kind of questions is False\n",
      "----- Macro AVG. -----\n",
      "[0.56, 0.49, 0.46, 0.41, 0.38]\n",
      "[0.54, 0.55, 0.46, 0.46, 0.39]\n",
      "Significancy for Macro AVG. is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/scipy/stats/morestats.py:1678: UserWarning: Input data for shapiro has range zero. The results may not be accurate.\n",
      "  warnings.warn(\"Input data for shapiro has range zero. The results \"\n",
      "/usr/local/lib/python3.6/dist-packages/statsmodels/stats/weightstats.py:650: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  tstat = (value1 - value2 - diff) / std_diff\n"
     ]
    }
   ],
   "source": [
    "for clm in list(bert_basic_res[0].keys()):\n",
    "    bert_seq_vals = [v[clm] if clm in v else 0 for v in bert_seq_res]\n",
    "    bert_basic_vals = [v[clm] if clm in v else 0 for v in bert_basic_res]\n",
    "    print('----- {} -----'.format(clm))\n",
    "    print(bert_basic_vals)\n",
    "    print(bert_seq_vals)\n",
    "    print('Significancy for {} is {}'.format(clm, check_sig(bert_seq_vals, bert_basic_vals, alpha=0.05)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Explanation move labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "results_table = [ \n",
    "    eval_preds(df_slice, ['Majority', 'BERT', 'BERT-SEQ', 'BERT-SEQ'], ['exp_act_label', 'exp_act_label', 'exp_act_label', 'exp_act_label'], \n",
    "                           ['exp_act_maj_pred', 'exp_act_label_bert_pred', 'exp_act_label_seq_bert_pred', 'exp-act-lable_mt_pred'])\n",
    "    for df_slice in np.array_split(mace_annotation_preds, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- (E02) Testing prior knowledge -----\n",
      "[0.47, 0.77, 0.73, 0.52, 0.67]\n",
      "[0.5, 0.71, 0.72, 0.62, 0.62]\n",
      "Significancy for (E02) Testing prior knowledge is False\n",
      "----- (E03) Provide an explanation -----\n",
      "[0.88, 0.81, 0.82, 0.84, 0.86]\n",
      "[0.88, 0.8, 0.82, 0.83, 0.85]\n",
      "Significancy for (E03) Provide an explanation is False\n",
      "----- (E05) Signaling understanding -----\n",
      "[0.19, 0.3, 0.23, 0.33, 0.34]\n",
      "[0.19, 0.53, 0.05, 0.39, 0.14]\n",
      "Significancy for (E05) Signaling understanding is False\n",
      "----- (E09) Introducing Extraneous Information -----\n",
      "[0.13, 0.22, 0.08, 0.0, 0]\n",
      "[0.15, 0.0, 0.19, 0.0, 0]\n",
      "Significancy for (E09) Introducing Extraneous Information is False\n",
      "----- (E07) Providing Feedback -----\n",
      "[0.63, 0.39, 0.54, 0.52, 0.51]\n",
      "[0.73, 0.42, 0.57, 0.62, 0.63]\n",
      "Significancy for (E07) Providing Feedback is True\n",
      "----- (E01) Testing understanding -----\n",
      "[0.2, 0.29, 0.35, 0.29, 0.22]\n",
      "[0.18, 0.27, 0.2, 0.25, 0.39]\n",
      "Significancy for (E01) Testing understanding is False\n",
      "----- (E10) Other -----\n",
      "[0.67, 0.41, 0.63, 0.35, 0.0]\n",
      "[0.67, 0.56, 0.86, 0.2, 0.0]\n",
      "Significancy for (E10) Other is False\n",
      "----- (E04) Ask for an explanation -----\n",
      "[0.52, 0.49, 0.68, 0.69, 0.62]\n",
      "[0.52, 0.57, 0.75, 0.72, 0.62]\n",
      "Significancy for (E04) Ask for an explanation is False\n",
      "----- (E08) Providing Assessment -----\n",
      "[0.0, 0, 0.0, 0, 0.0]\n",
      "[0.0, 0, 0.0, 0, 0.4]\n",
      "Significancy for (E08) Providing Assessment is False\n",
      "----- Macro AVG. -----\n",
      "[0.41, 0.41, 0.44, 0.44, 0.42]\n",
      "[0.43, 0.47, 0.44, 0.4, 0.42]\n",
      "Significancy for Macro AVG. is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/scipy/stats/morestats.py:2957: UserWarning: Exact p-value calculation does not work if there are ties. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/stats/morestats.py:2971: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    }
   ],
   "source": [
    "bert_seq_res   = [{x[0]: x[1]['f1'] for x in res_item[2][2].items()} for res_item in results_table]\n",
    "bert_basic_res = [{x[0]: x[1]['f1'] for x in res_item[1][2].items()} for res_item in results_table]\n",
    "\n",
    "for clm in list(bert_basic_res[0].keys()):\n",
    "    bert_seq_vals = [v[clm] if clm in v else 0 for v in bert_seq_res]\n",
    "    bert_basic_vals = [v[clm] if clm in v else 0 for v in bert_basic_res]\n",
    "    print('----- {} -----'.format(clm))\n",
    "    print(bert_basic_vals)\n",
    "    print(bert_seq_vals)\n",
    "    print('Significancy for {} is {}'.format(clm, check_sig(bert_seq_vals, bert_basic_vals, alpha=0.1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
